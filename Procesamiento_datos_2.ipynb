{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método para extraer Carácteristicas\n",
    "\n",
    "El método get_vector_features es el encargado de analizar los registros en un intervalo de tiempo y extraer el vector de 11 carácteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_features(trafic_device,label):\n",
    "    \n",
    "    trafic_device = trafic_device.reset_index()\n",
    "    #Calcule Active/Sleep Time by IoT generate traffic in every Second\n",
    "    start_time = trafic_device.iloc[0]['TIME']\n",
    "    end_time = trafic_device.iloc[-1]['TIME']\n",
    "    #print(start_time)\n",
    "    #print(end_time)\n",
    "    #print(end_time-start_time)\n",
    "    \n",
    "    active_time = []\n",
    "    sleep_time = []\n",
    "    mean_rate = []\n",
    "    f_trigger = 0\n",
    "    count_sleep_time = 0\n",
    "    count_active_time = 0\n",
    "    for time in range(start_time,end_time+1):\n",
    "        #The traces of the second i\n",
    "        second_traces = trafic_device[trafic_device[\"TIME\"] == time]\n",
    "        if len(second_traces)>0:            \n",
    "            #Se deberían incluir los 0 ? porque si no hay tráfico no se debería analizar.\n",
    "            mean_rate.append(second_traces[\"Size\"].sum())\n",
    "            #It's a trigger ?\n",
    "            trigger_traces = second_traces[(second_traces['port.dst'] != 53) & \n",
    "                                           (second_traces['port.dst'] != 123) &\n",
    "                                           (second_traces['port.src'] != 53) & \n",
    "                                           (second_traces['port.src'] != 123)\n",
    "                                          ]\n",
    "            if len(trigger_traces)>0:\n",
    "                f_trigger+= 1\n",
    "            \n",
    "            #Reset sleep time count\n",
    "            if count_sleep_time > 0:\n",
    "                sleep_time.append(count_sleep_time)\n",
    "                count_sleep_time = 0\n",
    "            count_active_time +=1\n",
    "        else:\n",
    "            #Reset active time count\n",
    "            if count_active_time > 0:\n",
    "                active_time.append(count_active_time)\n",
    "                count_active_time = 0\n",
    "            count_sleep_time+=1\n",
    "    \n",
    "    #Calcule Active Volume\n",
    "    f_trigger = f_trigger/(end_time - start_time)\n",
    "    x_frame = 79 #Maximum frame overhead\n",
    "    x_payload = 1 #Heartbeat payload\n",
    "    active_volume = f_trigger*(x_frame+x_payload)\n",
    "    \n",
    "    #N servers and protocols\n",
    "    n_servers = trafic_device[(trafic_device['IP.dst'] != \"255.255.255.255\")]\n",
    "    n_protocols = trafic_device[\"IP.proto\"].nunique()\n",
    "    \n",
    "    #Calcule DNS Interval and Unique Request\n",
    "    dns_request = trafic_device[(trafic_device['port.dst'] == 53)]\n",
    "    \n",
    "    dns_interval = []\n",
    "    \n",
    "    if len(dns_request) > 1:\n",
    "        times_dns_interval = []\n",
    "        for index, row in dns_request.iterrows():\n",
    "            times_dns_interval.append(row[\"TIME\"])\n",
    "        for i in range(len(times_dns_interval)-1):\n",
    "            dns_interval.append(times_dns_interval[i+1]-times_dns_interval[i])\n",
    "            \n",
    "    else:\n",
    "        dns_interval.append(0)\n",
    "    \n",
    "    #Calcule NTP Traffic\n",
    "    ntp_interval = []\n",
    "    \n",
    "    npt_request = trafic_device[(trafic_device['port.dst'] == 123)]\n",
    "    \n",
    "    if len(npt_request) > 1:\n",
    "        times_ntp_interval = []\n",
    "        for index, row in npt_request.iterrows():\n",
    "            times_ntp_interval.append(row[\"TIME\"])\n",
    "        for i in range(len(times_ntp_interval)-1):\n",
    "            ntp_interval.append(times_ntp_interval[i+1]-times_ntp_interval[i])\n",
    "            \n",
    "    else:\n",
    "        ntp_interval.append(0)\n",
    "    \n",
    "    \n",
    "    #Features\n",
    "    if np.sum(active_time) > 0:\n",
    "        mean_active_time = round(np.mean(active_time),3)\n",
    "    else:\n",
    "        mean_active_time = 0\n",
    "    \n",
    "    \n",
    "    if np.sum(sleep_time) > 0:\n",
    "        mean_sleep_time = round(np.mean(sleep_time),3)\n",
    "    else:\n",
    "        mean_sleep_time = 0\n",
    "    \n",
    "    if trafic_device[\"Size\"].sum() > 0:\n",
    "        size_mean = round(trafic_device[\"Size\"].mean(),3)\n",
    "    else: \n",
    "        size_mean = 0\n",
    "    \n",
    "    if np.sum(mean_rate) > 0:\n",
    "        mean_mean_rate = round(np.mean(mean_rate),3)\n",
    "        peak_mean_rate = round(np.max(mean_rate)/np.mean(mean_rate),3)\n",
    "    else:\n",
    "        mean_mean_rate = 0\n",
    "        peak_mean_rate = 0\n",
    "    \n",
    "    if np.sum(dns_interval) > 0:\n",
    "        mean_dns_interval = round(np.mean(dns_interval),3)\n",
    "    else:\n",
    "        mean_dns_interval = 0\n",
    "    \n",
    "    if np.sum(ntp_interval) > 0:\n",
    "        mean_ntp_interval = round(np.mean(ntp_interval),3)\n",
    "    else:\n",
    "        mean_ntp_interval = 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Generate Feature Vectors.\n",
    "    feature = [mean_active_time,mean_sleep_time,active_volume,size_mean,\n",
    "               mean_mean_rate,peak_mean_rate,n_servers['device_dest'].nunique(),n_protocols,dns_request['IP.dst'].nunique(),\n",
    "               mean_dns_interval,mean_ntp_interval]\n",
    "    \n",
    "    #Remove Inf values\n",
    "    for i in range(len(feature)):\n",
    "        if feature[i] == np.inf:\n",
    "            feature[i]=0\n",
    "    \n",
    "    return feature\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método para procesar archivo csv\n",
    "\n",
    "Este metodo permite procesar un archivo csv, se encarga de analizar etiqueta por etiqueta extrallendo el intervalo de tiempo donde se encuentran registros del dispositivo en el archivo, posteriormente calcula si puede generar intervalos de 5 minutos como mínimo (300 segundos) para extraer un vector de carácteristicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crude_csv(day,total_vectors=48,time_step=300):    \n",
    "    \n",
    "    #Parameters\n",
    "    #48*11= 528\n",
    "    #seg - 5 min    \n",
    "    \n",
    "    print(\"Day: \"+day)\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    dataset = pd.read_csv('dataset/crude/f-'+day)\n",
    "    \n",
    "    for label in range(1,22):\n",
    "\n",
    "        data_device = dataset[(dataset[\"device_src\"] == label) | (dataset[\"device_dest\"] == label)].loc[:,\"Packet ID\":]\n",
    "        data_device = data_device.sort_values(by=['TIME'])\n",
    "        total_vector_generate = 0\n",
    "        vector_generate = 0\n",
    "        print(\"label: \"+str(label))\n",
    "        \n",
    "        if len(data_device) >0:\n",
    "\n",
    "            start_time =  data_device.iloc[0]['TIME']\n",
    "            end_time = data_device.iloc[-1]['TIME']\n",
    "            \n",
    "\n",
    "            total_seconds = end_time - start_time\n",
    "            \n",
    "            iterations = int(total_seconds/time_step)\n",
    "\n",
    "            feature = [label]\n",
    "\n",
    "            for i in range(0,iterations):\n",
    "                for windows in range(0,300,60):                   \n",
    "                    if ((start_time+((i+1)*time_step)+windows) <= end_time):\n",
    "                        #print(\"windows: \"+str(windows))\n",
    "                        #print(\"i \"+str(i))\n",
    "                        #print(\"start: \"+str((start_time+(i*time_step)+windows)))\n",
    "                        #print(\"end: \" + str((start_time+((i+1)*time_step)+windows)))\n",
    "                        data_windows_time = data_device[(data_device.TIME >= (start_time+(i*time_step)+windows)) & (data_device.TIME<(start_time+((i+1)*time_step)+windows))]\n",
    "                        if len(data_windows_time)>0:\n",
    "                            vector = get_vector_features(data_windows_time,label)\n",
    "                            feature = np.concatenate((feature,vector))\n",
    "                            vector_generate+=1\n",
    "                            total_vector_generate+=1\n",
    "                        if vector_generate == 48:\n",
    "                            features.append(feature.tolist())\n",
    "                            vector_generate = 0\n",
    "                            feature = [label]\n",
    "\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "        \n",
    "        print(\"Features: \"+str(int(total_vector_generate/total_vectors)))\n",
    "\n",
    "    generate_df = pd.DataFrame(features)\n",
    "    \n",
    "    return generate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesar 20 días \n",
    "En las siguientes lineas se hace el llamado dia por día y se procesa cada día extrallendo vectores de carácteristicas para cada dispositivo, posteriormente se guarda como un archivo procesado csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day: 16-09-23.csv\n",
      "label: 1\n",
      "Features: 29\n",
      "label: 2\n",
      "Features: 29\n",
      "label: 3\n",
      "Features: 29\n",
      "label: 4\n"
     ]
    }
   ],
   "source": [
    "days = ['16-09-23.csv','16-09-24.csv','16-09-25.csv','16-09-26.csv','16-09-27.csv','16-09-28.csv',\n",
    "        '16-09-29.csv','16-09-30.csv','16-10-01.csv','16-10-02.csv','16-10-03.csv','16-10-04.csv',\n",
    "        '16-10-05.csv','16-10-06.csv','16-10-07.csv','16-10-08.csv','16-10-09.csv','16-10-10.csv',\n",
    "        '16-10-11.csv','16-10-12.csv']\n",
    "\n",
    "for day in days:    \n",
    "    generate_df = read_crude_csv(day)\n",
    "    generate_df.to_csv('dataset/p-'+day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.118, 8.529, 9.411764705882353, 72.757, 86.322, 4.425, 1, 2, 1, 0, 0]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "- https://erg.abdn.ac.uk/users/gorry/course/lan-pages/enet-calc.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "column_names =  ['active_time', 'sleep_time', 'active_volume',\n",
    "                 'avg_pack_size','mean_rate','peak_mean_rate','n_servers',\n",
    "                 'n_proto','unique_dns_request','dns_interval','ntp_interval','dispositive']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
